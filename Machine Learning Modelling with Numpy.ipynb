{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e4c85c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      baseline value  accelerations  fetal_movement  uterine_contractions  \\\n",
      "0              120.0          0.000           0.000                 0.000   \n",
      "1              132.0          0.006           0.000                 0.006   \n",
      "2              133.0          0.003           0.000                 0.008   \n",
      "3              134.0          0.003           0.000                 0.008   \n",
      "4              132.0          0.007           0.000                 0.008   \n",
      "...              ...            ...             ...                   ...   \n",
      "2121           140.0          0.000           0.000                 0.007   \n",
      "2122           140.0          0.001           0.000                 0.007   \n",
      "2123           140.0          0.001           0.000                 0.007   \n",
      "2124           140.0          0.001           0.000                 0.006   \n",
      "2125           142.0          0.002           0.002                 0.008   \n",
      "\n",
      "      light_decelerations  severe_decelerations  prolongued_decelerations  \\\n",
      "0                   0.000                   0.0                       0.0   \n",
      "1                   0.003                   0.0                       0.0   \n",
      "2                   0.003                   0.0                       0.0   \n",
      "3                   0.003                   0.0                       0.0   \n",
      "4                   0.000                   0.0                       0.0   \n",
      "...                   ...                   ...                       ...   \n",
      "2121                0.000                   0.0                       0.0   \n",
      "2122                0.000                   0.0                       0.0   \n",
      "2123                0.000                   0.0                       0.0   \n",
      "2124                0.000                   0.0                       0.0   \n",
      "2125                0.000                   0.0                       0.0   \n",
      "\n",
      "      abnormal_short_term_variability  mean_value_of_short_term_variability  \\\n",
      "0                                73.0                                   0.5   \n",
      "1                                17.0                                   2.1   \n",
      "2                                16.0                                   2.1   \n",
      "3                                16.0                                   2.4   \n",
      "4                                16.0                                   2.4   \n",
      "...                               ...                                   ...   \n",
      "2121                             79.0                                   0.2   \n",
      "2122                             78.0                                   0.4   \n",
      "2123                             79.0                                   0.4   \n",
      "2124                             78.0                                   0.4   \n",
      "2125                             74.0                                   0.4   \n",
      "\n",
      "      percentage_of_time_with_abnormal_long_term_variability  ...  \\\n",
      "0                                                  43.0       ...   \n",
      "1                                                   0.0       ...   \n",
      "2                                                   0.0       ...   \n",
      "3                                                   0.0       ...   \n",
      "4                                                   0.0       ...   \n",
      "...                                                 ...       ...   \n",
      "2121                                               25.0       ...   \n",
      "2122                                               22.0       ...   \n",
      "2123                                               20.0       ...   \n",
      "2124                                               27.0       ...   \n",
      "2125                                               36.0       ...   \n",
      "\n",
      "      histogram_min  histogram_max  histogram_number_of_peaks  \\\n",
      "0              62.0          126.0                        2.0   \n",
      "1              68.0          198.0                        6.0   \n",
      "2              68.0          198.0                        5.0   \n",
      "3              53.0          170.0                       11.0   \n",
      "4              53.0          170.0                        9.0   \n",
      "...             ...            ...                        ...   \n",
      "2121          137.0          177.0                        4.0   \n",
      "2122          103.0          169.0                        6.0   \n",
      "2123          103.0          170.0                        5.0   \n",
      "2124          103.0          169.0                        6.0   \n",
      "2125          117.0          159.0                        2.0   \n",
      "\n",
      "      histogram_number_of_zeroes  histogram_mode  histogram_mean  \\\n",
      "0                            0.0           120.0           137.0   \n",
      "1                            1.0           141.0           136.0   \n",
      "2                            1.0           141.0           135.0   \n",
      "3                            0.0           137.0           134.0   \n",
      "4                            0.0           137.0           136.0   \n",
      "...                          ...             ...             ...   \n",
      "2121                         0.0           153.0           150.0   \n",
      "2122                         0.0           152.0           148.0   \n",
      "2123                         0.0           153.0           148.0   \n",
      "2124                         0.0           152.0           147.0   \n",
      "2125                         1.0           145.0           143.0   \n",
      "\n",
      "      histogram_median  histogram_variance  histogram_tendency  fetal_health  \n",
      "0                121.0                73.0                 1.0           2.0  \n",
      "1                140.0                12.0                 0.0           1.0  \n",
      "2                138.0                13.0                 0.0           1.0  \n",
      "3                137.0                13.0                 1.0           1.0  \n",
      "4                138.0                11.0                 1.0           1.0  \n",
      "...                ...                 ...                 ...           ...  \n",
      "2121             152.0                 2.0                 0.0           2.0  \n",
      "2122             151.0                 3.0                 1.0           2.0  \n",
      "2123             152.0                 4.0                 1.0           2.0  \n",
      "2124             151.0                 4.0                 1.0           2.0  \n",
      "2125             145.0                 1.0                 0.0           1.0  \n",
      "\n",
      "[2126 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('fetal_health.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1bee18e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline value                                            0\n",
      "accelerations                                             0\n",
      "fetal_movement                                            0\n",
      "uterine_contractions                                      0\n",
      "light_decelerations                                       0\n",
      "severe_decelerations                                      0\n",
      "prolongued_decelerations                                  0\n",
      "abnormal_short_term_variability                           0\n",
      "mean_value_of_short_term_variability                      0\n",
      "percentage_of_time_with_abnormal_long_term_variability    0\n",
      "mean_value_of_long_term_variability                       0\n",
      "histogram_width                                           0\n",
      "histogram_min                                             0\n",
      "histogram_max                                             0\n",
      "histogram_number_of_peaks                                 0\n",
      "histogram_number_of_zeroes                                0\n",
      "histogram_mode                                            0\n",
      "histogram_mean                                            0\n",
      "histogram_median                                          0\n",
      "histogram_variance                                        0\n",
      "histogram_tendency                                        0\n",
      "fetal_health                                              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking for null_values\n",
    "null_values = df.isnull().sum()\n",
    "#count null values for eachn column\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4c56ea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (2126, 21)\n",
      "Shape of y: (2126,)\n"
     ]
    }
   ],
   "source": [
    "#Define training Set\n",
    "X = df.iloc[:, :-1]  # All columns except the last one\n",
    "y = df.iloc[:, -1]   # Only the last column\n",
    "\n",
    "\n",
    "# Displaying the shapes of X and y to verify\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "36a414c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.35222005 -0.8223883  -0.20320955 -1.48246456 -0.63843755 -0.0574756\n",
      " -0.2687543   1.51319018 -0.94309501  1.80254152 -1.02856029 -0.1655066\n",
      " -1.06856207 -2.11959194 -0.70139685 -0.45844382 -1.06561383  0.15326971\n",
      " -1.18164215  1.87056871  1.11298001]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "#scaling the features for more efficient gradient descent\n",
    "\n",
    "# Initialize the class\n",
    "scaler_linear = StandardScaler()\n",
    "\n",
    "x = scaler_linear.fit_transform(X)\n",
    "\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "07717863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training set (input) is: (1275, 21)\n",
      "the shape of the training set (target) is: (1275,)\n",
      "\n",
      "the shape of the cross validation set (input) is: (425, 21)\n",
      "the shape of the cross validation set (target) is: (425,)\n",
      "\n",
      "the shape of the test set (input) is: (426, 21)\n",
      "the shape of the test set (target) is: (426,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#splitting training set for model evaluation after fitting is done\n",
    "\n",
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\n",
    "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)\n",
    "\n",
    "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fefcd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model \n",
    "#the neural network is a softmax model with one layer \n",
    "#i.e outputs g(z_1)...(g(z_(number of activations)))\n",
    "#the model in equation is given by can be given by g(z_1) = e(z_1)/(e(z_1) +...e(z_n)).... g(z_n) = e(z_n)/(e(z_1) +...e(z_n))\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    ez = np.exp(z)              #element-wise exponenial\n",
    "    sm = ez/np.sum(ez)\n",
    "    return(sm)\n",
    "\n",
    "def my_dense(a_in, W, b):\n",
    "    \"\"\"\n",
    "    Computes dense layer\n",
    "    Args:\n",
    "      a_in (ndarray (n, )) : Data, 1 example \n",
    "      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units\n",
    "      b    (ndarray (j, )) : bias vector, j units  \n",
    "    Returns\n",
    "      a_out (ndarray (j,))  : j units|\n",
    "    \"\"\"\n",
    "    units = W.shape[1]\n",
    "    z_out = np.matmul(a_in,W)\n",
    "    #matrix multiplication of input features and weight paramters(n.j)\n",
    "    a_out = softmax(z_out)\n",
    "    return(a_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5a8d1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify cost function and gradient descent to get most suitable parameters\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#one hot encoding y[i] to set the target to 1 and all other classes to 0\n",
    "encoder = OneHotEncoder(sparse=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5c280633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic(X, y_enc, w, b):\n",
    "    \"\"\"\n",
    "    Computes cost\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,n)) : target values\n",
    "      w (ndarray (n,j)) : model parameters  \n",
    "      b (ndarray)  (j,)     : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    units = w.shape[1]\n",
    "    cost = 0.0\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    for i in range(m):\n",
    "        #iterate through each training example\n",
    "        z = np.matmul(X[i],w)\n",
    "        #matrix multiplication of input features of each training example and weight paramters(n.j)\n",
    "        f_wb_i = softmax(z)\n",
    "        \n",
    "        #calculate loss for each training example that is given by the dot product of the y-encoded fr that \n",
    "        #training example(1d vector) and log(f_wb_i) for that training example(1d vector)\n",
    "        \n",
    "        cost += -(np.dot(y_enc[i],np.log(f_wb_i)))\n",
    "             \n",
    "    cost = cost / m\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bb4cdde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda2\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#test cost function\n",
    "vector = np.array([1, 2, 3])\n",
    "vector_encoded = encoder.fit_transform(vector.reshape(-1, 1))\n",
    "print(y_encoded[0])\n",
    "\n",
    "# Create a 3x3 matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])\n",
    "\n",
    "cost = compute_cost_logistic(matrix, vector_encoded, matrix, vector)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2987e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train parameters i.e run gradient descent to find best w and b that reduces the cost the most\n",
    "def compute_gradient_logistic(X, y_enc, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,n)): target values\n",
    "      w (ndarray (n,j)): model parameters  \n",
    "      b (vector)  (n,)    : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,j)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (vector)  (n.)    : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    units = w.shape[1]\n",
    "    dj_dw = np.zeros((n,units))#(n,)\n",
    "    dj_db = np.zeros(units)\n",
    "    z = np.zeros(units)\n",
    "    \n",
    "    \n",
    "   \n",
    "    #one hot encoding y[i] to set the target to 1 and all other classes to 0\n",
    "\n",
    "    for i in range(m):\n",
    "        #iterating through each example\n",
    "        z = np.matmul(X[i],w)\n",
    "        #matrix multiplication of input features of each training example and weight paramters(n.j)\n",
    "        f_wb = softmax(z)\n",
    "        #error for all units or activation\n",
    "        err = f_wb - y_enc[i]\n",
    "        for index,unit_err in enumerate(err):\n",
    "            #iterating through each activation error\n",
    "            for k in range(n):\n",
    "                #iterating through each feature in each training example and multiply by that activation error\n",
    "                #and store in n of W(n.j)\n",
    "                dj_dw[k,index] += unit_err * X[i,k] #(sum over all training examples)\n",
    "            dj_db[index] += unit_err #(sum over all training examples)\n",
    "\n",
    "    dj_dw = dj_dw/m                                   \n",
    "    dj_db = dj_db/m                                   \n",
    "        \n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b9d2f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.33333129 -0.33250903  0.66584032]\n"
     ]
    }
   ],
   "source": [
    "#test gradient function\n",
    "dj,dw = compute_gradient_logistic(matrix, vector_encoded, matrix, vector)\n",
    "print(dj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d86e8ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def gradient_descent(X, y_enc, w_in, b_in, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,j)): Initial values of model parameters  \n",
    "      b_in (ndarray (n,))     : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,j))   : Updated values of parameters\n",
    "      b (ndarray(n,))         : Updated value of parameter \n",
    "    \"\"\"\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = w_in  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y_enc, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y_enc, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9eb2c358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda2\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[0. 0. 0.]\n",
      "Iteration    0: Cost 1.0613743119820704   \n",
      "Iteration 1000: Cost 0.7019270080076103   \n",
      "Iteration 2000: Cost 0.6991955477085571   \n",
      "Iteration 3000: Cost 0.6981991279881226   \n",
      "Iteration 4000: Cost 0.697513866887841   \n",
      "Iteration 5000: Cost 0.6969596768047015   \n",
      "Iteration 6000: Cost 0.696478130357526   \n",
      "Iteration 7000: Cost 0.6960433836588407   \n",
      "Iteration 8000: Cost 0.6956426982220274   \n",
      "Iteration 9000: Cost 0.6952693803561969   \n",
      "[[-0.64819785 -0.11348069  0.76167854]\n",
      " [ 0.0842043  -0.49538381  0.41117951]\n",
      " [ 0.050053   -0.00244436 -0.04760865]\n",
      " [ 0.38165127 -0.2410268  -0.14062447]\n",
      " [ 0.24815317 -0.08012542 -0.16802775]\n",
      " [-1.97084944  1.01207772  0.95877172]\n",
      " [-1.44972559  0.70577665  0.74394894]\n",
      " [-0.54539999  0.26588419  0.27951581]\n",
      " [ 0.00465428 -0.02235528  0.017701  ]\n",
      " [-0.68768869  0.27659502  0.41109367]\n",
      " [-0.13424941  0.03199047  0.10225895]\n",
      " [ 0.15631107 -0.0797872  -0.07652386]\n",
      " [-0.55028035  0.28308809  0.26719226]\n",
      " [-0.56715862  0.29312999  0.27402863]\n",
      " [-0.08036621  0.19797987 -0.11761366]\n",
      " [ 0.02504441 -0.04772531  0.02268089]\n",
      " [ 0.6142226  -0.19367363 -0.42054897]\n",
      " [-0.34991326  0.52606002 -0.17614675]\n",
      " [ 1.07813324 -0.14024918 -0.93788406]\n",
      " [-0.58469249  0.27068904  0.31400345]\n",
      " [-0.42497253  0.17702254  0.24794999]]\n",
      "[ 272.15235295 -117.28847406 -154.86387889]\n",
      "\n",
      "updated parameters: w:[[-0.64819785 -0.11348069  0.76167854]\n",
      " [ 0.0842043  -0.49538381  0.41117951]\n",
      " [ 0.050053   -0.00244436 -0.04760865]\n",
      " [ 0.38165127 -0.2410268  -0.14062447]\n",
      " [ 0.24815317 -0.08012542 -0.16802775]\n",
      " [-1.97084944  1.01207772  0.95877172]\n",
      " [-1.44972559  0.70577665  0.74394894]\n",
      " [-0.54539999  0.26588419  0.27951581]\n",
      " [ 0.00465428 -0.02235528  0.017701  ]\n",
      " [-0.68768869  0.27659502  0.41109367]\n",
      " [-0.13424941  0.03199047  0.10225895]\n",
      " [ 0.15631107 -0.0797872  -0.07652386]\n",
      " [-0.55028035  0.28308809  0.26719226]\n",
      " [-0.56715862  0.29312999  0.27402863]\n",
      " [-0.08036621  0.19797987 -0.11761366]\n",
      " [ 0.02504441 -0.04772531  0.02268089]\n",
      " [ 0.6142226  -0.19367363 -0.42054897]\n",
      " [-0.34991326  0.52606002 -0.17614675]\n",
      " [ 1.07813324 -0.14024918 -0.93788406]\n",
      " [-0.58469249  0.27068904  0.31400345]\n",
      " [-0.42497253  0.17702254  0.24794999]], b:[ 272.15235295 -117.28847406 -154.86387889]\n"
     ]
    }
   ],
   "source": [
    "y_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
    "m,n = x_train.shape\n",
    "i,j = y_encoded.shape\n",
    "w_tmp = np.zeros((n,j))\n",
    "b_tmp = np.zeros(j)\n",
    "alph = 0.1\n",
    "iters = 10000\n",
    "\n",
    "w_out, b_out, _ = gradient_descent(x_train, y_encoded, w_tmp, b_tmp, alph, iters) \n",
    "print(w_out)\n",
    "print(b_out)\n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "550e786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19294117647058823\n",
      "0.46588235294117647\n",
      "0.5187793427230047\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "#testing the model on the training data, cross-validation data to check for evaluate it's errors (if the predicted values \n",
    "#align with the target)\n",
    "#finding the error on differnt training sets will help us evaluate if the model has a problem of overfitting(high variance) or\n",
    "#a problem of underfitting(high bias)\n",
    "#if training error is very high then it has a problem of high bias\n",
    "#if cross validation is much higher than training error then it has the problem of high variance\n",
    "\n",
    "\n",
    "def evaluate_error(x_samp, y_samp):\n",
    "    y_pred = []\n",
    "    count = 0\n",
    "    error = 0\n",
    "    for x_in in x_samp:\n",
    "        x_out = my_dense(x_in, w_out, b_out)\n",
    "        y_pred.append((np.argmax(x_out)+ 1))\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] != y_train_samp.values[i]:\n",
    "            count +=1\n",
    "    error = count/len(y_pred)\n",
    "    return error\n",
    "train_error = evaluate_error(x_train, y_train)\n",
    "cv_error = evaluate_error(x_cv, y_cv)\n",
    "test_error =  evaluate_error(x_test, y_test)\n",
    "print(train_error)\n",
    "print(cv_error)\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ef467e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda2\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.84401403  0.4713793  -0.16034157 ...  0.64234865 -0.53536128\n",
      "  -0.26437707]\n",
      " [-0.53909042 -0.8223883   0.13973429 ...  1.7748084  -0.53536128\n",
      "  -0.06888834]\n",
      " [ 1.08716886 -0.8223883  -0.20320955 ... -0.60335709 -0.42662161\n",
      "   0.39317594]\n",
      " ...\n",
      " [-1.04729644  3.57642154 -0.20320955 ...  0.98208657 -0.53536128\n",
      "  -1.15296222]\n",
      " [-1.25057885 -0.04612774 -0.07460561 ... -0.82984904 -0.42662161\n",
      "   0.41094764]\n",
      " [-0.64073162  0.21262578 -0.20320955 ... -0.37686514 -0.53536128\n",
      "  -0.12220345]]\n",
      "Iteration    0: Cost 1.0705767180527315   \n",
      "Iteration 1000: Cost 0.7530869408169706   \n",
      "Iteration 2000: Cost 0.7513038093913915   \n",
      "Iteration 3000: Cost 0.7503722135136556   \n",
      "Iteration 4000: Cost 0.7495140342952283   \n",
      "Iteration 5000: Cost 0.7486914190698973   \n",
      "Iteration 6000: Cost 0.747902948530841   \n",
      "Iteration 7000: Cost 0.7471488309674976   \n",
      "Iteration 8000: Cost 0.746429160689353   \n",
      "Iteration 9000: Cost 0.7457438502347554   \n",
      "[[-0.12144817  0.23670113 -0.11525295]\n",
      " [ 0.14469748 -0.25698603  0.11228855]\n",
      " [ 0.06081182  0.01389577 -0.07470759]\n",
      " [ 0.28812657 -0.19405856 -0.09406801]\n",
      " [ 0.01551792 -0.10568266  0.09016474]\n",
      " [-2.5747259   1.25245903  1.32226687]\n",
      " [-1.69380956  0.6795429   1.01426666]\n",
      " [-0.52001016  0.15580333  0.36420683]\n",
      " [-0.10236051 -0.06121132  0.16357183]\n",
      " [-0.70830172  0.28456305  0.42373867]\n",
      " [-0.11341123  0.04813811  0.06527312]]\n",
      "[ 282.39468991 -120.36737063 -162.02731928]\n",
      "\n",
      "updated parameters: w:[[-0.12144817  0.23670113 -0.11525295]\n",
      " [ 0.14469748 -0.25698603  0.11228855]\n",
      " [ 0.06081182  0.01389577 -0.07470759]\n",
      " [ 0.28812657 -0.19405856 -0.09406801]\n",
      " [ 0.01551792 -0.10568266  0.09016474]\n",
      " [-2.5747259   1.25245903  1.32226687]\n",
      " [-1.69380956  0.6795429   1.01426666]\n",
      " [-0.52001016  0.15580333  0.36420683]\n",
      " [-0.10236051 -0.06121132  0.16357183]\n",
      " [-0.70830172  0.28456305  0.42373867]\n",
      " [-0.11341123  0.04813811  0.06527312]], b:[ 282.39468991 -120.36737063 -162.02731928]\n"
     ]
    }
   ],
   "source": [
    "#from the evaluation, we see the model does really well on the training set doesn't do as well on the cross validation set,\n",
    "#the difference in error depicts a problem of overfitting(high variance) where the parameters fit the training data really well\n",
    "#but doesn't do as well on new data\n",
    "#the solution to this is simplifying the model and one way (out of multiple ways) is by reducing the amount of features\n",
    "#using all features except histogram features\n",
    "\n",
    "y_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
    "x_train_new = x_train[:, :11]\n",
    "print(x_train_new)\n",
    "m,n = x_train_new.shape\n",
    "i,j = y_encoded.shape\n",
    "w_tmp = np.zeros((n,j))\n",
    "b_tmp = np.zeros(j)\n",
    "alph = 0.1\n",
    "iters = 10000\n",
    "\n",
    "w_out, b_out, _ = gradient_descent(x_train_new, y_encoded, w_tmp, b_tmp, alph, iters) \n",
    "print(w_out)\n",
    "print(b_out)\n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fad4af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12144817  0.23670113 -0.11525295]\n",
      " [ 0.14469748 -0.25698603  0.11228855]\n",
      " [ 0.06081182  0.01389577 -0.07470759]\n",
      " [ 0.28812657 -0.19405856 -0.09406801]\n",
      " [ 0.01551792 -0.10568266  0.09016474]\n",
      " [-2.5747259   1.25245903  1.32226687]\n",
      " [-1.69380956  0.6795429   1.01426666]\n",
      " [-0.52001016  0.15580333  0.36420683]\n",
      " [-0.10236051 -0.06121132  0.16357183]\n",
      " [-0.70830172  0.28456305  0.42373867]\n",
      " [-0.11341123  0.04813811  0.06527312]]\n"
     ]
    }
   ],
   "source": [
    "print(w_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3ae8d500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19137254901960785\n",
      "0.4470588235294118\n",
      "0.4953051643192488\n"
     ]
    }
   ],
   "source": [
    "x_cv_new = x_cv[:,:11]\n",
    "x_test_new = x_test[:,:11]\n",
    "\n",
    "train_error = evaluate_error(x_train_new, y_train)\n",
    "cv_error = evaluate_error(x_cv_new, y_cv)\n",
    "test_error =  evaluate_error(x_test_new, y_test)\n",
    "\n",
    "print(train_error)\n",
    "print(cv_error)\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363bc37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have reduced th error in the cv and test models,further alterations to the complexity of the model will \n",
    "#help reduce the difference in error between the training and the cross-validation set\n",
    "#Note: typically a baseline performance is established to measure how accurate the training and cross validation set is\n",
    "#i.e if the ideal performance of fetal mortality prediction is 70% that is 0.3 error, that is ued to compare to\n",
    "#the training and cross validation set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
